import json

# --- SPARK Prompt Generation Function ---
def generate_planning_prompt(document_topic, specific_information_needed, goal):
    """
    Generates a customizable 'Planning Prompt' based on the SPARK framework
    for handling limited context windows.

    Args:
        document_topic (str): The topic of the document to be processed.
        specific_information_needed (str): The specific information the user
                                           needs to extract from the document.
        goal (str): The overall goal of processing the document.

    Returns:
        str: The complete and customizable planning prompt.

    Raises:
        ValueError: If any input is empty or invalid.
    """
    if not all([document_topic, specific_information_needed, goal]):
        raise ValueError("All inputs (document topic, specific information, and goal) must be non-empty.")

    s_task = "Create a smart chunking plan for a 1,500-word document with 100-word processing limit."
    p_perspective = (
        "You are an Information Architect specializing in document analysis and content structuring. "
        f"You understand how to preserve semantic meaning across segmented text and maintain context flow. "
        f"I need to extract {specific_information_needed} while working within technical constraints."
    )
    a_examples = (
        'Good approach: "Chunk 1: Introduction + main thesis (100 words) - keeps core idea together"\n'
        'Bad approach: "Chunk 1: Words 1-100 (arbitrary cut that might split sentences)"'
    )
    r_reasoning = (
        "Include reasoning for chunk boundaries. Explain WHY you chose these specific chunk boundaries "
        "and how they preserve meaning."
    )

    prompt = (
        f"S: {s_task}\n\n"
        f"P: {p_perspective}\n\n"
        f"A: {a_examples}\n\n"
        f"Document topic: {document_topic}\n"
        f"Goal: {goal}\n\n"
        f"R: {r_reasoning}"
    )

    return prompt

# --- Simulated AI API Connection ---
def simulate_ai_api_call(prompt_text, api_key="YOUR_BLANK_API_KEY"):
    """
    Simulates sending a prompt to an AI model via an API.
    In a real application, this would use a library like 'requests'
    to connect to a service like OpenAI's API.

    Args:
        prompt_text (str): The prompt generated by our system.
        api_key (str): A placeholder for an actual API key.

    Returns:
        dict: A dictionary simulating the AI's response.

    Raises:
        ValueError: If the API key is invalid or missing.
    """
    if not api_key or api_key == "YOUR_BLANK_API_KEY":
        raise ValueError("A valid API key is required.")

    print(f"\n--- Simulating API Call to AI Model ---")
    print(f"Using API Key: {api_key[:5]}... (first 5 chars)")
    print(f"Sending prompt of {len(prompt_text)} characters.")

    # Note: Token count is approximated as 1 token per 4 characters
    # In a real scenario, use a library like 'tiktoken' for accurate tokenization
    # For example, using OpenAI's API:
    # import openai
    # openai.api_key = api_key
    # response = openai.chat.completions.create(
    #     model="gpt-4",
    #     messages=[
    #         {"role": "system", "content": "You are an Information Architect assisting with document chunking."},
    #         {"role": "user", "content": prompt_text}
    #     ],
    #     temperature=0.7,
    #     max_tokens=500
    # )
    # return response.json()

    # For this demo, return a predefined simulated response
    # TODO: In a real implementation, dynamically generate the response based on prompt inputs
    simulated_response_content = (
        "Based on your 1,500-word 'User Guide for New Software' and the goal to 'answer user questions efficiently', "
        "hereâ€™s a smart chunking plan for a 100-word context window:\n\n"
        "Chunk 1 (Words 1-98): Introduction and 'Getting Started' section. Reasoning: Crucial for initial user understanding, sets context.\n"
        "Chunk 2 (Words 99-197): Basic Features Overview. Reasoning: Logical progression from getting started, covers core functionality.\n"
        "Chunk 3 (Words 198-295): Advanced Features Introduction. Reasoning: Separates complex topics, allows focused querying.\n"
        "Chunk 4 (Words 296-390): Troubleshooting Common Issues. Reasoning: Directly addresses 'troubleshooting steps' from your goal, high utility.\n"
        "Chunk 5 (Words 391-487): Frequently Asked Questions (FAQs) - Part 1. Reasoning: Directly addresses 'FAQs' from your goal, common user queries.\n"
        "Chunk 6 (Words 488-585): Frequently Asked Questions (FAQs) - Part 2. Reasoning: Continues critical FAQ information, ensuring comprehensive coverage.\n"
        "... (and so on for the remaining chunks up to 1500 words)\n\n"
        "This semantic chunking approach ensures that each segment contains a coherent piece of information, "
        "minimizing the risk of splitting crucial context across arbitrary word count boundaries."
    )

    simulated_ai_response = {
        "id": "chatcmpl-demo12345",
        "object": "chat.completion",
        "created": 1700000000,
        "model": "gpt-4",
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": simulated_response_content,
                },
                "logprobs": None,
                "finish_reason": "stop"
            }
        ],
        "usage": {
            "prompt_tokens": len(prompt_text) // 4,
            "completion_tokens": len(simulated_response_content) // 4,
            "total_tokens": (len(prompt_text) + len(simulated_response_content)) // 4
        }
    }
    print("AI response simulated successfully!")
    return simulated_ai_response

# --- Demo Usage ---
if __name__ == "__main__":
    print("Welcome to the Automated Prompt Generator & AI Connector!")
    print("Let's create and send a planning prompt for your document analysis to a simulated AI.\n")

    try:
        user_document_topic = input("Enter the topic of your document (e.g., 'User Guide for New Software'): ")
        user_specific_info = input("What specific information do you need to extract? (e.g., 'troubleshooting steps and FAQs'): ")
        user_goal = input("What is your overall goal for processing this document? (e.g., 'answer user questions efficiently'): ")

        # 1. Generate the prompt
        print("\n--- Step 1: Generating the SPARK Planning Prompt ---")
        generated_prompt = generate_planning_prompt(
            user_document_topic,
            user_specific_info,
            user_goal
        )
        print("Prompt generated successfully:")
        print(generated_prompt)
        print("\n--- End of Generated Prompt ---")

        # 2. Simulate sending the prompt to AI API
        print("\n--- Step 2: Connecting to AI and Sending Prompt ---")
        ai_response = simulate_ai_api_call(generated_prompt)

        # 3. Process and display AI's response
        print("\n--- Step 3: AI's Simulated Response ---")
        if ai_response and ai_response.get('choices'):
            ai_content = ai_response['choices'][0]['message']['content']
            print("\nAI Assistant's Chunking Plan:")
            print(ai_content)
            print(f"\nSimulated Token Usage: {ai_response['usage']['total_tokens']} tokens")
        else:
            print("No valid response received from simulated AI.")

    except ValueError as e:
        print(f"Error: {e}")
    except Exception as e:
        print(f"Unexpected error: {e}")

    print("\n--- Demo Complete ---")
    print("This demo illustrates how you can automatically generate tailored prompts")
    print("and then programmatically send them to an AI model via its API to automate workflows.")
